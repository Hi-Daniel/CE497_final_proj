{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Deep Cognitive Features from Bridge Inspectors Eye Tracking Data\n",
    "\n",
    "Previous to this study we collected data from five different PhD engineering students on the task of inspecting a bridge in a virtual environment. The students were given 3 minutes to inspect a bridge inside of a Unity platform that we have developed, and take images of any defects, cracks, or areas of concern that they found on the bridge. During their inspection we collected data of their character's movement as well as eye movement. Below you see an example of the type of data that was obtained at one time step:\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<Data>\n",
    "  <GazeData>\n",
    "    <DisplayDimensions Width=\"1920\" Height=\"1080\" />\n",
    "    <Timestamp>902049965729</Timestamp>\n",
    "    <GazeOrigin>\n",
    "      <CombinedGazeRayScreen Origin=\"(7656.03600000, 927.89240000, -1049.60500000)\" Direction=\"(-0.95211820, -0.28659020, 0.10647600)\" Valid=\"True\" />\n",
    "    </GazeOrigin>\n",
    "    <pupil average_pupildiameter=\"4.157799\" />\n",
    "    <IntersectionPoint X=\"4420.004\" Y=\"-46.16226\" Z=\"-687.7166\" />\n",
    "    <HitObject Name=\"RW_Bridge_Vologda_II_track_LOD0\">\n",
    "      <ObjectPosition X=\"0\" Y=\"0\" Z=\"0\" />\n",
    "    </HitObject>\n",
    "    <PositionOnDisplayArea X=\"0.5182106\" Y=\"0.5221348\" />\n",
    "  </GazeData>\n",
    "  <CameraData>\n",
    "    <Timestamp>902049965729</Timestamp>\n",
    "    <CameraOrigin X=\"7656.323\" Y=\"927.9785\" Z=\"-1049.637\" />\n",
    "    <CameraDirection X=\"-0.9624248\" Y=\"-0.2623872\" Z=\"0.06994079\" />\n",
    "  </CameraData>\n",
    "<Data>\n",
    "```\n",
    "\n",
    "One can see information such as the time of the data point, the location and viewing direction of the inspector at this time as well as the location where their eye were looking. This data was collected at a rate of 60 fps, therefore for 3 minutes of data we have 5400 data points.\n",
    "\n",
    "### Goal\n",
    "\n",
    "Our goal in this experiment is to determine whether we can use machine learning to make insights into the cognitive behavior of the inspectors based on this eye tracking data. For example, a successful insight would be if we can correctly identify whether a person is planning, searching, or deciding at every point in their search. For example, when the person is first planning where on the bridge to look, then the person might be actively looking, once the person finds something interesting such as a crack they will be deciding whether this is something they should take a picture of or not.\n",
    "\n",
    "In addition to these fine-scaled, granular behavioral patterns, we would also like to see if machine learning methods can identify more \"big picture\" patterns, such as classifying search styles based on a larger set of data.\n",
    "\n",
    "We believe that the insights gathered from these methods can be used as tools to make concrete data-driven decisions for designing training procedures for new inspectors, or comparing the efficiency of different inspection patterns.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "In order to extract deep features from our data we propose two methods. \n",
    "\n",
    "The first method consists of using dimensionality reduction and clustering techniques such as PCA, t-SNE, and k-means on manually extracted feature vectors, such as velocity, acceleration, rate of fixations to saccades, etc. Then investigating these reduced dimensionality arrays to determine which combination of features creates the most meaningful feature space.\n",
    "\n",
    "The second method involves using similar dimensionality reduction techniques on feature vectors extracted from unsupervised feature extraction techniques such as Deep Autoencoders, Variational Autoencoders, LSTM Autoencoders, etc.\n",
    "\n",
    "We will then visualize the two methods and extract clusters of points, then we will interpret the clusters to find whether they correspond to certain parts of the inspection process or different search patterns. We can apply both of these methods on a global as well as local level to get different types of clustering.\n",
    "\n",
    "For example, if we apply the method \"locally\" meaning that we divide the dataset into 1 second intervals and perform the clustering on these 1 second intervals then we expect the clusters to represent local/short term cognitive behavior such as whether a person is searching or planning.\n",
    "\n",
    "On the other hand, if we perform the clustering \"globally\" meaning that we use the entire data record we expect the clusters to represent more global characteristics about the search, such as search strategy, or who the inspector is.\n",
    "\n",
    "Something to keep in mind is that we want to anonymize the data records as much as possible to avoid clustering based on trivial factors such as the exact locations that inspectors looked at or other features that wouldn't generalize well and wouldn't truly represent an inspector's cognitive behavior.\n",
    "\n",
    "### Table of Contents (TO-DO)\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>Loading Data</li>\n",
    "    <li>Local clustering</li>\n",
    "        <ul>\n",
    "            <li>Breaking up the data</li>\n",
    "            <li>Method 1\n",
    "                <ul>\n",
    "                    <li>Extracting manual features</li>\n",
    "                    <li>Trying different clustering techniques</li>\n",
    "                    <li>Visualizing clusters</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Method 2\n",
    "                <ul>\n",
    "                    <li>Training Deep AE, VAE, and LSTM AE</li>\n",
    "                    <li>Extracting deep features</li>\n",
    "                    <li>Clustering / dimensionality reduction of deep features</li>\n",
    "                    <li>Visualizing clusters</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Testing generality of clusters</li>\n",
    "            <li>Findings and conclusions</li>\n",
    "        </ul>\n",
    "    <li>Global clustering\n",
    "        <ul>\n",
    "            <li>Extracting manual global features</li>\n",
    "            <li>Extracting deep features (Training extractor model)</li>\n",
    "            <li>Clustering techniques</li>\n",
    "            <li>Visualizing clusters</li>\n",
    "            <li>Testing generality of clusters</li>\n",
    "            <li>Findings and conclusions</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data <a name=\"loading-data\"></a>\n",
    "\n",
    "In this section we will load the data from the different users' xml files.\n",
    "\n",
    "The data is located in the test_1 folder. The files are divided into two data records for each user, the first data record is for the task of bridge inspection (with suffix \"_truss.xml\"). The second data record is for the task of looking for a certain object inside a warehouse (with suffix \"_warehouse.xml\"). We will use the second data record for testing the generality of our clusters, but not for generating the classifiers themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm # Use notebook version for better display\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 XML files in test_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5e24483cb84372b40ddb9d1a0b89a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading XML files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'null'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m DATA_FOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Make sure this folder exists relative to the notebook\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m truss_data, warehouse_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display some basic info about the loaded data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTruss Data Keys:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(truss_data\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "File \u001b[1;32mc:\\Users\\xavie\\Documents\\2025\\02 UIUC SPRING 2025\\02 CEE498 ML\\03 Project\\02 GitHub Docs\\eyeDataViz\\CE497_final_proj\\data_loader.py:124\u001b[0m, in \u001b[0;36mload_all_data\u001b[1;34m(data_folder)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(xml_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading XML files\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    123\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path)\n\u001b[1;32m--> 124\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mparse_xml_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;66;03m# Extract user ID and task type from filename (adjust pattern if needed)\u001b[39;00m\n\u001b[0;32m    127\u001b[0m         parts \u001b[38;5;241m=\u001b[39m file_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xavie\\Documents\\2025\\02 UIUC SPRING 2025\\02 CEE498 ML\\03 Project\\02 GitHub Docs\\eyeDataViz\\CE497_final_proj\\data_loader.py:54\u001b[0m, in \u001b[0;36mparse_xml_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m     hit_object \u001b[38;5;241m=\u001b[39m gaze\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//HitObject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     46\u001b[0m     pos_display \u001b[38;5;241m=\u001b[39m gaze\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//PositionOnDisplayArea\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m     gaze_dict[ts] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(ts),\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGazeOrigin\u001b[39m\u001b[38;5;124m'\u001b[39m: parse_vector(combined_gaze\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrigin\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m combined_gaze \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mnan]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGazeDirection\u001b[39m\u001b[38;5;124m'\u001b[39m: parse_vector(combined_gaze\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDirection\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m combined_gaze \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mnan]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGazeValid\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_gaze\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m combined_gaze \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvgPupilDiameter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pupil\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_pupildiameter\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m pupil \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m pupil\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_pupildiameter\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[1;32m---> 54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntersectionPoint\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43m[\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mintersection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mintersection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHitObjectName\u001b[39m\u001b[38;5;124m'\u001b[39m: hit_object\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m hit_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjectPosition\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mfloat\u001b[39m(hit_object\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//ObjectPosition\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget(ax)) \u001b[38;5;28;01mif\u001b[39;00m hit_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hit_object\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//ObjectPosition\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositionOnDisplayX\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pos_display\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m pos_display \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositionOnDisplayY\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pos_display\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m pos_display \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     59\u001b[0m     }\n\u001b[0;32m     61\u001b[0m camera_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m camera \u001b[38;5;129;01min\u001b[39;00m camera_entries:\n",
      "File \u001b[1;32mc:\\Users\\xavie\\Documents\\2025\\02 UIUC SPRING 2025\\02 CEE498 ML\\03 Project\\02 GitHub Docs\\eyeDataViz\\CE497_final_proj\\data_loader.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m     hit_object \u001b[38;5;241m=\u001b[39m gaze\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//HitObject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     46\u001b[0m     pos_display \u001b[38;5;241m=\u001b[39m gaze\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//PositionOnDisplayArea\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m     gaze_dict[ts] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(ts),\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGazeOrigin\u001b[39m\u001b[38;5;124m'\u001b[39m: parse_vector(combined_gaze\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrigin\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m combined_gaze \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mnan]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGazeDirection\u001b[39m\u001b[38;5;124m'\u001b[39m: parse_vector(combined_gaze\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDirection\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m combined_gaze \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mnan]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGazeValid\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_gaze\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m combined_gaze \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvgPupilDiameter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pupil\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_pupildiameter\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m pupil \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m pupil\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_pupildiameter\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[1;32m---> 54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntersectionPoint\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mfloat\u001b[39m(intersection\u001b[38;5;241m.\u001b[39mget(ax)) \u001b[38;5;28;01mif\u001b[39;00m intersection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHitObjectName\u001b[39m\u001b[38;5;124m'\u001b[39m: hit_object\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m hit_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjectPosition\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mfloat\u001b[39m(hit_object\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//ObjectPosition\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget(ax)) \u001b[38;5;28;01mif\u001b[39;00m hit_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hit_object\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//ObjectPosition\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositionOnDisplayX\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pos_display\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m pos_display \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositionOnDisplayY\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pos_display\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m pos_display \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     59\u001b[0m     }\n\u001b[0;32m     61\u001b[0m camera_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m camera \u001b[38;5;129;01min\u001b[39;00m camera_entries:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'null'"
     ]
    }
   ],
   "source": [
    "# Set plot style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "# Specify the folder containing the XML data\n",
    "DATA_FOLDER = \"test_1\" # Make sure this folder exists relative to the notebook\n",
    "# Load the data\n",
    "truss_data, warehouse_data = data_loader.load_all_data(DATA_FOLDER)\n",
    "\n",
    "# Display some basic info about the loaded data\n",
    "print(\"\\nTruss Data Keys:\", list(truss_data.keys()))\n",
    "if truss_data:\n",
    "    # Example: Display head of the first truss dataset\n",
    "    first_key = list(truss_data.keys())[0]\n",
    "    print(f\"\\nHead of data for '{first_key}':\")\n",
    "    display(truss_data[first_key].head())\n",
    "    print(f\"\\nInfo for '{first_key}':\")\n",
    "    truss_data[first_key].info()\n",
    "\n",
    "print(\"\\nWarehouse Data Keys:\", list(warehouse_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Clustering\n",
    "### Breaking up the data <a name=\"break-up-data\"></a>\n",
    "\n",
    "For performing local clustering we would like to gather features that represent cognitive behavior at one instance in time. For example, is the person planning right now or are they actively searching? To achieve this we propose to first break up our data records into small instances of time (such as 1s) and then perform the clustering and subsequent analysis on these short time records.\n",
    "\n",
    "Equally as important, we must anonymize these short time records in order to avoid clustering based on trivial facts. For example, if not anonymized, the clustering analysis could yield clusters that simply divide the data based on location (Such as looking at the top of the deck vs the bottom) but these clusters do not provide any insight into the person's cognitive behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "WINDOW_SECONDS = 1.0 # Duration of each local window\n",
    "SAMPLING_RATE = 60 # Hz (data points per second)\n",
    "WINDOW_POINTS = int(WINDOW_SECONDS * SAMPLING_RATE)\n",
    "\n",
    "print(f\"Window size: {WINDOW_SECONDS} seconds ({WINDOW_POINTS} data points)\")\n",
    "\n",
    "\n",
    "# Process all truss data\n",
    "all_windows_anonymized = {}\n",
    "for key, df in tqdm(truss_data.items(), desc=\"Creating & Anonymizing Windows\"):\n",
    "    windows_raw = data_loader.create_windows(df, WINDOW_POINTS)\n",
    "    # Filter out windows that might be too short if create_windows logic changes\n",
    "    windows_raw = [w for w in windows_raw if len(w) == WINDOW_POINTS]\n",
    "    anonymized_windows = [data_loader.anonymize_window(w) for w in windows_raw]\n",
    "    all_windows_anonymized[key] = anonymized_windows\n",
    "    print(f\"Processed '{key}': Created {len(windows_raw)} raw windows -> {len(anonymized_windows)} anonymized windows.\")\n",
    "\n",
    "# Example: Display the first anonymized window for the first user\n",
    "if all_windows_anonymized:\n",
    "    first_key = list(all_windows_anonymized.keys())[0]\n",
    "    if all_windows_anonymized[first_key]:\n",
    "        print(f\"\\nHead of first anonymized window for '{first_key}':\")\n",
    "        display(all_windows_anonymized[first_key][0].head())\n",
    "        print(f\"\\nColumns in anonymized window:\")\n",
    "        print(all_windows_anonymized[first_key][0].columns)\n",
    "    else:\n",
    "        print(f\"No windows created for '{first_key}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "### Extracting manual features <a name=\"manual-feature-extraction\"></a>\n",
    "\n",
    "After anonymizing the data records we will do some manual feature engineering on our data in order to provide more meaningful features for our clustering algorithms. Based on previous literature we have decided to include the following features:\n",
    "\n",
    "1. Eye metrics\n",
    "   - Average pupil diameter (mm)\n",
    "   - Average eye movement velocity (deg/s)\n",
    "   - Average eye movement acceleration (deg/s/s)\n",
    "   - Ratio of fixation/saccade (s)\n",
    "   - Number of fixations\n",
    "   - Average fixation duration (s)\n",
    "   - Spatial variance of fixations (deg)\n",
    "   - Mean saccade velocity (deg/s)\n",
    "   - Mean saccade amplitude (deg)\n",
    "1. Movement patterns (within 1s window)\n",
    "   - Average movement velocity (m/s)\n",
    "   - Average turning velocity (deg/s)\n",
    "   - Total action, including translation and rotation\n",
    "\n",
    "These features are selected to capture cognitive behavior patterns\n",
    "while being meaningful within a 1-second analysis window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_manual_features_for_window(window_df):\n",
    "    \"\"\"\n",
    "    Calculates the 14 manual features for a single anonymized window (DataFrame).\n",
    "    This function will eventually live in feature_engineering.py\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    if window_df.empty or len(window_df) < 2:\n",
    "        # Return default values or NaNs if window is too short\n",
    "        # Define default/NaN structure based on expected feature names\n",
    "        return {\n",
    "             # Eye metrics\n",
    "            'avg_pupil_diameter': np.nan, 'avg_eye_velocity_degps': np.nan,\n",
    "            'avg_eye_accel_degps2': np.nan, 'fix_sacc_ratio': np.nan,\n",
    "            'n_fixations': 0, 'avg_fixation_duration': np.nan,\n",
    "            'fixation_spatial_variance': np.nan, 'mean_saccade_velocity': np.nan,\n",
    "            'mean_saccade_amplitude': np.nan, 'gaze_area_covered': np.nan,\n",
    "            'gaze_spatial_density': np.nan,\n",
    "             # Movement patterns\n",
    "            'avg_movement_velocity_mps': np.nan, 'avg_turning_velocity_degps': np.nan,\n",
    "            'total_action': np.nan\n",
    "        }\n",
    "\n",
    "    # Placeholder calculations - Replace with actual implementations\n",
    "    # based on the anonymized columns (e.g., GazeTurningRate, CameraSpeed)\n",
    "    # and potentially fixation/saccade detection algorithms.\n",
    "\n",
    "    # 1. Eye metrics\n",
    "    features['avg_pupil_diameter'] = window_df['AvgPupilDiameter'].mean()\n",
    "\n",
    "    # --- Requires Fixation/Saccade Detection ---\n",
    "    # Placeholder: Need an algorithm (e.g., I-VT, I-DT) to classify points\n",
    "    # For now, use placeholders or simple proxies if possible\n",
    "    # Example proxy: Use GazeTurningRate threshold\n",
    "    saccade_threshold_degps = 100 # Example threshold\n",
    "    window_df['is_saccade'] = window_df['GazeTurningRate'].abs() > saccade_threshold_degps\n",
    "    window_df['is_fixation'] = ~window_df['is_saccade']\n",
    "\n",
    "    # Avg eye movement velocity (using GazeTurningRate as proxy)\n",
    "    features['avg_eye_velocity_degps'] = window_df['GazeTurningRate'].abs().mean()\n",
    "    # Avg eye movement acceleration (numerically differentiate velocity)\n",
    "    eye_accel = window_df['GazeTurningRate'].diff().abs() / window_df['dt']\n",
    "    features['avg_eye_accel_degps2'] = eye_accel.mean()\n",
    "\n",
    "    # Fixation/Saccade ratio (duration based)\n",
    "    total_fixation_time = window_df.loc[window_df['is_fixation'], 'dt'].sum()\n",
    "    total_saccade_time = window_df.loc[window_df['is_saccade'], 'dt'].sum()\n",
    "    features['fix_sacc_ratio'] = total_fixation_time / total_saccade_time if total_saccade_time > 0 else np.inf\n",
    "\n",
    "    # --- Fixation-based features (Placeholder implementation) ---\n",
    "    # Need to group consecutive fixation points into fixation events\n",
    "    # This requires more complex logic (event detection)\n",
    "    features['n_fixations'] = window_df['is_fixation'].sum() # Simplistic: count fixation points\n",
    "    features['avg_fixation_duration'] = total_fixation_time / features['n_fixations'] if features['n_fixations'] > 0 else 0\n",
    "    features['fixation_spatial_variance'] = np.nan # Requires fixation locations\n",
    "    features['mean_saccade_velocity'] = window_df.loc[window_df['is_saccade'], 'GazeTurningRate'].abs().mean() # Proxy\n",
    "    features['mean_saccade_amplitude'] = np.nan # Requires saccade start/end points\n",
    "    features['gaze_area_covered'] = np.nan # Requires gaze points (e.g., IntersectionPoint or PositionOnDisplay)\n",
    "    features['gaze_spatial_density'] = np.nan # Requires gaze points\n",
    "\n",
    "    # 2. Movement patterns\n",
    "    features['avg_movement_velocity_mps'] = window_df['CameraSpeed'].mean()\n",
    "    features['avg_turning_velocity_degps'] = window_df['CameraTurningRate'].abs().mean()\n",
    "    # Total action: Combine translation and rotation (needs weighting/scaling)\n",
    "    # Example: simple sum of mean speeds (needs normalization/better definition)\n",
    "    normalized_speed = features['avg_movement_velocity_mps'] / (window_df['CameraSpeed'].max() + 1e-6)\n",
    "    normalized_turn = features['avg_turning_velocity_degps'] / (window_df['CameraTurningRate'].abs().max() + 1e-6)\n",
    "    features['total_action'] = normalized_speed + normalized_turn # Simplistic combination\n",
    "\n",
    "    # Clean up NaNs/Infs resulting from calculations\n",
    "    for key, value in features.items():\n",
    "        if pd.isna(value) or np.isinf(value):\n",
    "            features[key] = 0 # Replace with 0 or another suitable default\n",
    "\n",
    "    return features\n",
    "\n",
    "# Calculate features for all anonymized windows\n",
    "all_manual_features = []\n",
    "window_indices = [] # Keep track of which user/window each feature row corresponds to\n",
    "\n",
    "for key, anonymized_windows in tqdm(all_windows_anonymized.items(), desc=\"Calculating Manual Features\"):\n",
    "    for i, window_df in enumerate(anonymized_windows):\n",
    "        features = calculate_manual_features_for_window(window_df)\n",
    "        all_manual_features.append(features)\n",
    "        window_indices.append({'user_task': key, 'window_index': i})\n",
    "\n",
    "# Create DataFrames\n",
    "manual_features_df = pd.DataFrame(all_manual_features)\n",
    "window_indices_df = pd.DataFrame(window_indices)\n",
    "\n",
    "# Combine indices with features\n",
    "manual_features_df = pd.concat([window_indices_df, manual_features_df], axis=1)\n",
    "\n",
    "print(\"\\nManual Features DataFrame:\")\n",
    "display(manual_features_df.head())\n",
    "print(\"\\nInfo:\")\n",
    "manual_features_df.info()\n",
    "print(\"\\nDescription:\")\n",
    "display(manual_features_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different clustering techniques <a name=\"clustering1\"></a>\n",
    "\n",
    "In this section we will apply clustering techniques to the manual features created above. We will apply the following techniques and explore the clusters created.\n",
    "\n",
    "1. PCA\n",
    "2. T-SNE\n",
    "3. K-means\n",
    "4. Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Select only the numeric feature columns for clustering\n",
    "feature_columns = manual_features_df.select_dtypes(include=np.number).columns\n",
    "# Drop columns that might be constant or have zero variance if they exist\n",
    "# Also drop index/identifier columns if they were included by mistake\n",
    "feature_columns = feature_columns.drop(['window_index'], errors='ignore')\n",
    "\n",
    "print(f\"Using features for clustering: {feature_columns.tolist()}\")\n",
    "\n",
    "# Handle potential NaN/Inf values if not already done during feature extraction\n",
    "features_for_clustering = manual_features_df[feature_columns].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features_for_clustering)\n",
    "\n",
    "print(f\"Scaled features shape: {scaled_features.shape}\")\n",
    "\n",
    "# --- 1. PCA (Dimensionality Reduction & Visualization) ---\n",
    "print(\"\\n--- Performing PCA ---\")\n",
    "pca = PCA(n_components=2) # Reduce to 2 dimensions for visualization\n",
    "pca_result = pca.fit_transform(scaled_features)\n",
    "print(f\"Explained variance ratio (first 2 components): {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], alpha=0.6)\n",
    "plt.title('PCA of Manual Features (First 2 Components)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 2. T-SNE (Dimensionality Reduction & Visualization) ---\n",
    "print(\"\\n--- Performing T-SNE ---\")\n",
    "# Note: T-SNE can be computationally expensive on large datasets.\n",
    "# Consider using a subset or PCA first if it's too slow.\n",
    "n_samples_tsne = min(5000, scaled_features.shape[0]) # Limit samples for performance\n",
    "indices = np.random.choice(scaled_features.shape[0], n_samples_tsne, replace=False)\n",
    "scaled_features_subset = scaled_features[indices, :]\n",
    "# pca_result_subset = pca_result[indices, :] # Can apply t-SNE on PCA results too\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42, verbose=1)\n",
    "tsne_result = tsne.fit_transform(scaled_features_subset) # Use subset for T-SNE\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=tsne_result[:, 0], y=tsne_result[:, 1], alpha=0.6)\n",
    "plt.title(f't-SNE of Manual Features ({n_samples_tsne} samples)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 3. K-Means Clustering ---\n",
    "print(\"\\n--- Performing K-Means Clustering ---\")\n",
    "# Determine optimal K using the Elbow method\n",
    "inertia = []\n",
    "k_range = range(1, 11) # Check K from 1 to 10\n",
    "for k in k_range:\n",
    "    kmeans_test = KMeans(n_clusters=k, random_state=42, n_init=10) # Suppress warning\n",
    "    kmeans_test.fit(scaled_features)\n",
    "    inertia.append(kmeans_test.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (Within-cluster sum of squares)')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose K based on the elbow plot (e.g., K=4)\n",
    "optimal_k = 4 # Adjust based on the plot\n",
    "print(f\"Selected K = {optimal_k}\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Add cluster labels to the original features DataFrame (optional)\n",
    "manual_features_df['kmeans_cluster'] = kmeans_labels\n",
    "\n",
    "# Visualize K-Means clusters using PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=kmeans_labels, palette='viridis', alpha=0.7, s=50)\n",
    "plt.title(f'K-Means Clustering (K={optimal_k}) Results on PCA Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 4. Hierarchical Clustering ---\n",
    "print(\"\\n--- Performing Hierarchical Clustering ---\")\n",
    "# Generate the linkage matrix using Ward's method (minimizes variance within clusters)\n",
    "# Use a subset if the dataset is very large, as linkage calculation can be heavy\n",
    "n_samples_hc = min(2000, scaled_features.shape[0])\n",
    "indices_hc = np.random.choice(scaled_features.shape[0], n_samples_hc, replace=False)\n",
    "scaled_features_subset_hc = scaled_features[indices_hc, :]\n",
    "\n",
    "linked = linkage(scaled_features_subset_hc, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(linked,\n",
    "           orientation='top',\n",
    "           # labels=manual_features_df.index[indices_hc].tolist(), # Optional: add labels if meaningful/small subset\n",
    "           distance_sort='descending',\n",
    "           show_leaf_counts=True,\n",
    "           truncate_mode='lastp', # Show only the last p merged clusters\n",
    "           p=30) # Adjust 'p' to control dendrogram complexity\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward, Truncated)')\n",
    "plt.xlabel('Sample Index (or Cluster Size)')\n",
    "plt.ylabel('Distance (Ward)') \n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Choose the number of clusters based on the dendrogram (e.g., by cutting at a certain height)\n",
    "num_hierarchical_clusters = 4 # Adjust based on dendrogram inspection\n",
    "print(f\"Selected number of hierarchical clusters = {num_hierarchical_clusters}\")\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "agg_cluster = AgglomerativeClustering(n_clusters=num_hierarchical_clusters, affinity='euclidean', linkage='ward')\n",
    "hierarchical_labels = agg_cluster.fit_predict(scaled_features) # Fit on all data\n",
    "\n",
    "# Add cluster labels to the original features DataFrame (optional)\n",
    "manual_features_df['hierarchical_cluster'] = hierarchical_labels\n",
    "\n",
    "# Visualize Hierarchical clusters using PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=hierarchical_labels, palette='viridis', alpha=0.7, s=50)\n",
    "plt.title(f'Hierarchical Clustering (n={num_hierarchical_clusters}) Results on PCA Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualizing clusters <a name=\"visualize-clusters1\"></a>\n",
    "\n",
    "We will visualize the different clusters created from the algorithms and evaluate whether these clusters correspond to specific cognitive patterns, or any other patterns observed between the different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "\n",
    "### Deep feature extraction and Clustering <a name=\"deep-feature-extraction\"></a>\n",
    "\n",
    "In this section we will use unsupervised deep learning approaches like Autoencoders, Variational Autoencoders (VAE), and LSTM Autoencoders to extract latent features from the raw data or the manually selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring deep feature clusters\n",
    "\n",
    "We will again use algorithms such as PCA, t-SNE, k-means and hierarchical clustering to cluster the deep features and visualize the latent space. As done before we will evaluate whether these clusters correspond to specific patterns in the inspections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing generality of clusters\n",
    "\n",
    "To test the generality of these cluster we will load the warehouse data. For this dataset the user was asked to find a specific graffiti inside a warehouse full of graffiti. Although the task is very different in nature, the cognitive patterns should be similar, for example the clusters should be capable of identifying when the person is looking at a graffiti and deciding if it is the correct one or not. On the other hand, we might find that the clusters generated don't generalize well to this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
